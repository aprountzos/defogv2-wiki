"use strict";(globalThis.webpackChunkdefogv_2_wiki=globalThis.webpackChunkdefogv_2_wiki||[]).push([[5496],{7432:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"architecture/components/metrics-monitor","title":"Metrics Monitor","description":"Lightweight multi-metric monitoring for stress, RPS, health, and cluster capacity with adaptive thresholds","source":"@site/docs/architecture/components/metrics-monitor.mdx","sourceDirName":"architecture/components","slug":"/architecture/components/metrics-monitor","permalink":"/defogv2-wiki/docs/architecture/components/metrics-monitor","draft":false,"unlisted":false,"editUrl":"https://github.com/aprountzos/defogv2-wiki/tree/main/docs/docs/architecture/components/metrics-monitor.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Metrics Monitor","description":"Lightweight multi-metric monitoring for stress, RPS, health, and cluster capacity with adaptive thresholds","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Discovery Controller","permalink":"/defogv2-wiki/docs/architecture/components/discovery-controller"},"next":{"title":"Scheduler","permalink":"/defogv2-wiki/docs/architecture/components/scheduler"}}');var t=n(4848),i=n(8453);const l={title:"Metrics Monitor",description:"Lightweight multi-metric monitoring for stress, RPS, health, and cluster capacity with adaptive thresholds",sidebar_position:2},o="Metrics Monitor",c={},a=[{value:"Overview",id:"overview",level:2},{value:"Purpose",id:"purpose",level:2},{value:"Collection Strategy",id:"collection-strategy",level:2},{value:"Concurrent Collection with Goroutines",id:"concurrent-collection-with-goroutines",level:3},{value:"Strategy 1: Stress Monitoring",id:"strategy-1-stress-monitoring",level:3},{value:"Strategy 2: Adaptive RPS Monitoring",id:"strategy-2-adaptive-rps-monitoring",level:3},{value:"Strategy 3: Health Monitoring",id:"strategy-3-health-monitoring",level:3},{value:"Strategy 4: Cluster Capacity",id:"strategy-4-cluster-capacity",level:3},{value:"Collection Process",id:"collection-process",level:2},{value:"Concurrent Collection Flow",id:"concurrent-collection-flow",level:3},{value:"Efficiency Optimizations",id:"efficiency-optimizations",level:3},{value:"Configuration",id:"configuration",level:2},{value:"ConfigMap Settings",id:"configmap-settings",level:3},{value:"Per-Service Overrides",id:"per-service-overrides",level:3},{value:"API Endpoints",id:"api-endpoints",level:2},{value:"GET /metrics/stress",id:"get-metricsstress",level:3},{value:"GET /metrics/rps",id:"get-metricsrps",level:3},{value:"GET /metrics/health",id:"get-metricshealth",level:3},{value:"GET /metrics/capacity",id:"get-metricscapacity",level:3},{value:"GET /metrics/snapshot",id:"get-metricssnapshot",level:3},{value:"GET /metrics/stats",id:"get-metricsstats",level:3},{value:"Events Emitted",id:"events-emitted",level:2},{value:"StressEvent",id:"stressevent",level:3},{value:"HighRPSEvent",id:"highrpsevent",level:3},{value:"LowRPSEvent",id:"lowrpsevent",level:3},{value:"Deployment",id:"deployment",level:2},{value:"Operational Characteristics",id:"operational-characteristics",level:2},{value:"Resource Usage (Edge-Optimized)",id:"resource-usage-edge-optimized",level:3},{value:"Performance",id:"performance",level:3},{value:"Key Optimizations",id:"key-optimizations",level:3},{value:"Monitoring",id:"monitoring",level:2},{value:"Prometheus Metrics",id:"prometheus-metrics",level:3},{value:"Alerts",id:"alerts",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Issue: No Stress Events Detected",id:"issue-no-stress-events-detected",level:3},{value:"Issue: No RPS Events Detected",id:"issue-no-rps-events-detected",level:3},{value:"Issue: High Memory Usage",id:"issue-high-memory-usage",level:3},{value:"Issue: Adaptive Thresholds Not Working",id:"issue-adaptive-thresholds-not-working",level:3},{value:"Advanced Features",id:"advanced-features",level:2},{value:"Adaptive Threshold Calculation",id:"adaptive-threshold-calculation",level:3},{value:"Hot Node Detection",id:"hot-node-detection",level:3},{value:"Event Throttling",id:"event-throttling",level:3},{value:"Composite Stress Scoring",id:"composite-stress-scoring",level:3},{value:"Health-Based Alerting",id:"health-based-alerting",level:3},{value:"Integration Points",id:"integration-points",level:2},{value:"With Orchestrator",id:"with-orchestrator",level:3},{value:"With Prometheus",id:"with-prometheus",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Source Code Reference",id:"source-code-reference",level:2},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"For Small Edge Clusters (&lt; 50 services)",id:"for-small-edge-clusters--50-services",level:3},{value:"For Medium Clusters (50-200 services)",id:"for-medium-clusters-50-200-services",level:3},{value:"For Large Clusters (200+ services)",id:"for-large-clusters-200-services",level:3},{value:"Migration Guide",id:"migration-guide",level:2},{value:"From Static to Adaptive Thresholds",id:"from-static-to-adaptive-thresholds",level:3},{value:"From v1 to v2 Monitor",id:"from-v1-to-v2-monitor",level:3},{value:"Related Documentation",id:"related-documentation",level:2}];function d(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"metrics-monitor",children:"Metrics Monitor"})}),"\n",(0,t.jsxs)(r.p,{children:["The Metrics Monitor continuously collects and analyzes metrics: ",(0,t.jsx)(r.strong,{children:"pod stress"})," (CPU/RAM usage), ",(0,t.jsx)(r.strong,{children:"service RPS"})," (requests per second), ",(0,t.jsx)(r.strong,{children:"health metrics"})," (error rates, latency), and ",(0,t.jsx)(r.strong,{children:"cluster capacity"}),". It uses adaptive thresholds and emits events when conditions are met, triggering the appropriate strategy."]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Optimized for edge deployments"})," - efficient resource usage while maintaining full observability."]}),"\n",(0,t.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(r.mermaid,{value:'graph TB\r\n    subgraph Sources\r\n        K8S[Kubernetes<br/>Metrics API]\r\n        PROM[Prometheus]\r\n    end\r\n    \r\n    subgraph Monitor["Monitor (Concurrent Collection)"]\r\n        SC[Stress Collector]\r\n        RC[RPS Collector]\r\n        HC[Health Collector]\r\n        CC[Capacity Collector]\r\n        HM[History Manager]\r\n    end\r\n    \r\n    K8S --\x3e SC\r\n    K8S --\x3e CC\r\n    PROM --\x3e RC\r\n    PROM --\x3e HC\r\n    \r\n    SC --\x3e HM\r\n    RC --\x3e HM\r\n    \r\n    SC --\x3e|StressEvent| ORCH[Orchestrator]\r\n    RC --\x3e|HighRPSEvent| ORCH\r\n    RC --\x3e|LowRPSEvent| ORCH\r\n    HC --\x3e|HealthEvent| ORCH\r\n    CC --\x3e|CapacityReport| ORCH\r\n    \r\n    style SC fill:#ffe1e1\r\n    style RC fill:#e1ffe1\r\n    style HC fill:#fff4e1\r\n    style CC fill:#e1f4ff'}),"\n",(0,t.jsx)(r.h2,{id:"purpose",children:"Purpose"}),"\n",(0,t.jsxs)(r.p,{children:["The Metrics Monitor is the ",(0,t.jsx)(r.strong,{children:"observability layer"})," of Defog v2, responsible for:"]}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Stress Monitoring"})," - Detect when pods are overloaded (composite CPU+Memory score)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Adaptive RPS Tracking"})," - Measure requests per second with percentile-based thresholds"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Health Monitoring"})," - Track error rates and latency per service"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Capacity Reporting"})," - Calculate available cluster resources and identify hot nodes"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Event Emission"})," - Notify Orchestrator when thresholds crossed"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"History Tracking"})," - Maintain time-series data for adaptive thresholds"]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"collection-strategy",children:"Collection Strategy"}),"\n",(0,t.jsx)(r.h3,{id:"concurrent-collection-with-goroutines",children:"Concurrent Collection with Goroutines"}),"\n",(0,t.jsxs)(r.p,{children:["All metrics are collected ",(0,t.jsx)(r.strong,{children:"in parallel"})," for maximum efficiency:"]}),"\n",(0,t.jsx)(r.mermaid,{value:"sequenceDiagram\r\n    participant M as Monitor\r\n    participant S as Stress Collector\r\n    participant R as RPS Collector\r\n    participant H as Health Collector\r\n    \r\n    par Concurrent Collection\r\n        M->>S: Collect Stress (goroutine 1)\r\n        M->>R: Collect RPS (goroutine 2)\r\n        M->>H: Collect Health (goroutine 3)\r\n    end\r\n    \r\n    S--\x3e>M: Stress Metrics\r\n    R--\x3e>M: RPS Metrics\r\n    H--\x3e>M: Health Metrics\r\n    \r\n    M->>M: Merge into ServiceSnapshot\r\n    M->>M: Analyze & Generate Events\r\n    \r\n    Note over M: Total time: ~500-800ms"}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Benefits:"})}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"3x faster than sequential collection"}),"\n",(0,t.jsx)(r.li,{children:"Sub-second collection cycles"}),"\n",(0,t.jsx)(r.li,{children:"Efficient resource usage"}),"\n"]}),"\n",(0,t.jsx)(r.h3,{id:"strategy-1-stress-monitoring",children:"Strategy 1: Stress Monitoring"}),"\n",(0,t.jsxs)(r.p,{children:["Tracks pod resource usage with ",(0,t.jsx)(r.strong,{children:"composite scoring"}),":"]}),"\n",(0,t.jsx)(r.mermaid,{value:"graph LR\r\n    A[Batch Get All Pods] --\x3e B[Get CPU/RAM Usage]\r\n    B --\x3e C[Calculate Per-Pod]\r\n    C --\x3e D[Composite Score<br/>CPU\xd70.7 + Mem\xd70.3]\r\n    D --\x3e E{Score > 0.80?}\r\n    E --\x3e|Yes| F[Emit StressEvent]\r\n    E --\x3e|No| G[Store in History]\r\n    \r\n    style F fill:#ffe1e1"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"What it tracks"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"CPU usage vs CPU limit (per pod)"}),"\n",(0,t.jsx)(r.li,{children:"Memory usage vs Memory limit (per pod)"}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Composite stress score"})," (weighted: 70% CPU, 30% Memory)"]}),"\n",(0,t.jsx)(r.li,{children:"Average stress across all pods"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Key improvements"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"\u2705 Batch K8s API calls (all pods at once)"}),"\n",(0,t.jsx)(r.li,{children:"\u2705 Informers/watches instead of polling"}),"\n",(0,t.jsx)(r.li,{children:"\u2705 Configurable weights per service"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Data structure"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:'type StressMetrics struct {\r\n    Pods           []PodStress\r\n    AvgCPUUsage    float64  // 0.0 - 1.0\r\n    AvgMemoryUsage float64  // 0.0 - 1.0\r\n    CompositeScore float64  // Weighted: CPU\xd70.7 + Mem\xd70.3\r\n    StressLevel    string   // "low", "medium", "high", "critical"\r\n}\n'})}),"\n",(0,t.jsx)(r.h3,{id:"strategy-2-adaptive-rps-monitoring",children:"Strategy 2: Adaptive RPS Monitoring"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"No static thresholds!"})," Uses percentile-based adaptive detection:"]}),"\n",(0,t.jsx)(r.mermaid,{value:"graph LR\r\n    A[Query Prometheus] --\x3e B[Calculate Current RPS]\r\n    B --\x3e C[Update History Buffer]\r\n    C --\x3e D[Calculate Percentiles<br/>P50, P95]\r\n    D --\x3e E{Check Adaptive<br/>Thresholds}\r\n    E --\x3e|Current > P95| F[Emit HighRPSEvent]\r\n    E --\x3e|Current < P50| G[Emit LowRPSEvent]\r\n    E --\x3e|Normal| H[Store Only]\r\n    \r\n    style F fill:#e1ffe1\r\n    style G fill:#ffffe1"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"How it works"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{children:"Example History: [100, 110, 95, 105, 98, 102, 110] RPS\r\n\r\nP50 (median) = 102 RPS\r\nP95 (95th percentile) = 110 RPS\r\n\r\nTrigger HighRPS when: current > P95 + (P95 \xd7 hysteresis)\r\n                     current > 110 + (110 \xd7 0.10)\r\n                     current > 121 RPS\r\n\r\nTrigger LowRPS when: current < P50 - (P50 \xd7 hysteresis)\r\n                    current < 102 - (102 \xd7 0.10)\r\n                    current < 92 RPS\n"})}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Benefits"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"\u2705 Adapts to each service's traffic patterns"}),"\n",(0,t.jsx)(r.li,{children:"\u2705 Works for 5 RPS or 5000 RPS services"}),"\n",(0,t.jsx)(r.li,{children:"\u2705 No manual threshold tuning needed"}),"\n",(0,t.jsx)(r.li,{children:"\u2705 Hysteresis prevents flapping"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"What it tracks"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Current RPS (real-time)"}),"\n",(0,t.jsx)(r.li,{children:"Average RPS (over time window)"}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"P50 & P95 percentiles"})," (for adaptive thresholds)"]}),"\n",(0,t.jsx)(r.li,{children:"Whether service is local or mirrored"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Data structure"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:"type RPSMetrics struct {\r\n    CurrentRPS float64\r\n    AvgRPS     float64\r\n    P50        float64  // Adaptive low threshold\r\n    P95        float64  // Adaptive high threshold\r\n}\n"})}),"\n",(0,t.jsx)(r.h3,{id:"strategy-3-health-monitoring",children:"Strategy 3: Health Monitoring"}),"\n",(0,t.jsx)(r.p,{children:"Tracks service health indicators:"}),"\n",(0,t.jsx)(r.mermaid,{value:"graph LR\r\n    A[Query Prometheus] --\x3e B[Get Error Rate<br/>5xx / total]\r\n    A --\x3e C[Get P95 Latency<br/>histogram_quantile]\r\n    B --\x3e D[Determine Health]\r\n    C --\x3e D\r\n    D --\x3e E{Healthy?}\r\n    E --\x3e|No & High RPS| F[Emit HealthEvent]\r\n    E --\x3e|Yes| G[Store Only]\r\n    \r\n    style F fill:#fff4e1"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"What it tracks"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Error rate (percentage of 5xx responses)"}),"\n",(0,t.jsx)(r.li,{children:"P95 latency (milliseconds)"}),"\n",(0,t.jsx)(r.li,{children:"Health status (healthy / degraded / unhealthy)"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Data structure"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:'type HealthMetrics struct {\r\n    ErrorRate  float64  // 0.0 - 1.0\r\n    P95Latency float64  // milliseconds\r\n    IsHealthy  bool\r\n    Status     string   // "healthy", "degraded", "unhealthy"\r\n}\n'})}),"\n",(0,t.jsx)(r.h3,{id:"strategy-4-cluster-capacity",children:"Strategy 4: Cluster Capacity"}),"\n",(0,t.jsx)(r.p,{children:"Tracks cluster-wide resources and hot nodes:"}),"\n",(0,t.jsx)(r.mermaid,{value:"graph LR\r\n    A[List All Nodes] --\x3e B[Get Node Metrics]\r\n    B --\x3e C[Calculate Total<br/>& Used Resources]\r\n    C --\x3e D[Identify Hot Nodes<br/>>80% util]\r\n    D --\x3e E[Determine CanScale<br/>>20% free]\r\n    \r\n    style E fill:#e1f4ff"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"What it tracks"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Total/Free CPU and Memory"}),"\n",(0,t.jsx)(r.li,{children:"CPU/Memory utilization (%)"}),"\n",(0,t.jsx)(r.li,{children:"Hot nodes (>80% utilization)"}),"\n",(0,t.jsx)(r.li,{children:"CanScale flag (>20% free resources)"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Data structure"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:"type ClusterState struct {\r\n    TotalCPU      float64\r\n    FreeCPU       float64\r\n    CPUUtilization float64\r\n    CanScale      bool\r\n    HotNodes      []NodeInfo  // Nodes >80% util\r\n}\n"})}),"\n",(0,t.jsx)(r.h2,{id:"collection-process",children:"Collection Process"}),"\n",(0,t.jsx)(r.h3,{id:"concurrent-collection-flow",children:"Concurrent Collection Flow"}),"\n",(0,t.jsx)(r.mermaid,{value:"sequenceDiagram\r\n    participant M as Metrics Monitor\r\n    participant K8S as Kubernetes API\r\n    participant PROM as Prometheus\r\n    participant ORCH as Orchestrator\r\n    \r\n    loop Every 30s\r\n        Note over M: Start concurrent collection\r\n        \r\n        par Collect in Parallel\r\n            M->>K8S: Batch: Get all pod metrics\r\n            K8S--\x3e>M: All pods at once\r\n            \r\n            M->>PROM: Query RPS for all services\r\n            PROM--\x3e>M: RPS data\r\n            \r\n            M->>PROM: Query error rates + latency\r\n            PROM--\x3e>M: Health data\r\n        end\r\n        \r\n        Note over M: Merge results (500-800ms)\r\n        \r\n        M->>M: Calculate stress scores\r\n        M->>M: Calculate P50/P95 thresholds\r\n        M->>M: Determine health status\r\n        \r\n        alt Stress > threshold\r\n            M->>ORCH: StressEvent\r\n            Note over ORCH: Trigger stress strategy\r\n        end\r\n        \r\n        alt RPS > P95 (mirrored)\r\n            M->>ORCH: HighRPSEvent\r\n            Note over ORCH: Deploy locally\r\n        end\r\n        \r\n        alt Health degraded\r\n            M->>ORCH: HealthEvent\r\n            Note over ORCH: Investigate\r\n        end\r\n    end"}),"\n",(0,t.jsx)(r.h3,{id:"efficiency-optimizations",children:"Efficiency Optimizations"}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Batch API Calls:"})}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:"// OLD: One call per pod (slow!)\r\nfor pod := range pods {\r\n    metrics := getPodMetrics(pod)  // 100 pods = 100 calls\r\n}\r\n\r\n// NEW: One batch call (fast!)\r\nallMetrics := getAllPodMetrics()  // 100 pods = 1 call\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Informers vs Polling:"})}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:"// OLD: Poll every 30s\r\nticker := time.NewTicker(30 * time.Second)\r\nfor range ticker.C {\r\n    pods := listPods()  // API call every cycle\r\n}\r\n\r\n// NEW: Watch with informers\r\ninformer.AddEventHandler(cache.ResourceEventHandlerFuncs{\r\n    AddFunc:    onPodAdd,     // Instant updates\r\n    UpdateFunc: onPodUpdate,  // No polling\r\n    DeleteFunc: onPodDelete,\r\n})\n"})}),"\n",(0,t.jsx)(r.h2,{id:"configuration",children:"Configuration"}),"\n",(0,t.jsx)(r.h3,{id:"configmap-settings",children:"ConfigMap Settings"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'apiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: defog-config\r\n  namespace: defog-system\r\ndata:\r\n  # Collection intervals\r\n  CHECK_INTERVAL: "30s"\r\n  RPS_HISTORY_WINDOW: "5m"\r\n  STRESS_HISTORY_WINDOW: "5m"\r\n  \r\n  # Stress thresholds\r\n  STRESS_MEDIUM: "0.70"\r\n  STRESS_HIGH: "0.80"\r\n  STRESS_CRITICAL: "0.90"\r\n  STRESS_HYSTERESIS: "0.05"  # 5% buffer\r\n  \r\n  # Composite stress weights (per service)\r\n  CPU_WEIGHT: "0.7"      # 70% weight to CPU\r\n  MEMORY_WEIGHT: "0.3"   # 30% weight to memory\r\n  \r\n  # Adaptive RPS settings\r\n  USE_ADAPTIVE_THRESHOLDS: "true"\r\n  ADAPTIVE_WINDOW: "20"            # Number of samples\r\n  RPS_HYSTERESIS: "0.10"           # 10% buffer\r\n  \r\n  # Static RPS thresholds (fallback if adaptive disabled)\r\n  HIGH_RPS_THRESHOLD: "300.0"\r\n  LOW_RPS_THRESHOLD: "10.0"\r\n  \r\n  # Health thresholds\r\n  ERROR_RATE_THRESHOLD: "0.05"     # 5% error rate\r\n  LATENCY_THRESHOLD: "1000.0"      # 1000ms P95 latency\r\n  \r\n  # Prometheus integration\r\n  PROMETHEUS_URL: "http://prometheus.monitoring.svc:9090"\r\n  RPS_QUERY: \'rate(http_requests_total{service="{{ .ServiceName }}"}[1m])\'\r\n  \r\n  # Event settings\r\n  EVENT_COOLDOWN: "2m"  # Minimum time between same events\r\n  \r\n  # Resource limits\r\n  MAX_TRACKED_SERVICES: "500"\r\n  MAX_HISTORY_SAMPLES: "50"  # Reduced for edge efficiency\r\n  \r\n  # Orchestrator endpoint\r\n  ORCHESTRATOR_URL: "http://orchestrator:8080"\n'})}),"\n",(0,t.jsx)(r.h3,{id:"per-service-overrides",children:"Per-Service Overrides"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:"# In the same ConfigMap\r\nservice_overrides:\r\n  critical-api:\r\n    stress_high: 0.70      # More sensitive\r\n    stress_critical: 0.85\r\n    cpu_weight: 0.8        # CPU-heavy workload\r\n    \r\n  payment-service:\r\n    stress_high: 0.75\r\n    cpu_weight: 0.5        # Memory-heavy workload\r\n    \r\n  background-worker:\r\n    stress_high: 0.90      # Less sensitive\r\n    stress_critical: 0.95\n"})}),"\n",(0,t.jsx)(r.h2,{id:"api-endpoints",children:"API Endpoints"}),"\n",(0,t.jsx)(r.h3,{id:"get-metricsstress",children:"GET /metrics/stress"}),"\n",(0,t.jsx)(r.p,{children:"Get stress metrics for a service."}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Query Parameters"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.code,{children:"service"})," - Service name (required)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.code,{children:"namespace"}),' - Namespace (default: "default")']}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Response"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "serviceName": "api",\r\n  "namespace": "default",\r\n  "podStress": [\r\n    {\r\n      "podName": "api-pod-1",\r\n      "cpuUsage": 0.85,\r\n      "memoryUsage": 0.70\r\n    },\r\n    {\r\n      "podName": "api-pod-2",\r\n      "cpuUsage": 0.78,\r\n      "memoryUsage": 0.65\r\n    }\r\n  ],\r\n  "avgCPUUsage": 0.815,\r\n  "avgMemoryUsage": 0.675,\r\n  "compositeScore": 0.773,\r\n  "stressLevel": "high",\r\n  "timestamp": "2024-01-15T10:30:00Z"\r\n}\n'})}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"get-metricsrps",children:"GET /metrics/rps"}),"\n",(0,t.jsx)(r.p,{children:"Get RPS metrics with adaptive percentiles for a service."}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Query Parameters"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.code,{children:"service"})," - Service name (required)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.code,{children:"namespace"}),' - Namespace (default: "default")']}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Response"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "serviceName": "api",\r\n  "namespace": "default",\r\n  "isLocal": true,\r\n  "currentRps": 450.0,\r\n  "avgRps": 420.0,\r\n  "p50": 400.0,\r\n  "p95": 500.0,\r\n  "timestamp": "2024-01-15T10:30:00Z"\r\n}\n'})}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"get-metricshealth",children:"GET /metrics/health"}),"\n",(0,t.jsx)(r.p,{children:"Get health metrics for a service."}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Query Parameters"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.code,{children:"service"})," - Service name (required)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.code,{children:"namespace"}),' - Namespace (default: "default")']}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Response"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "serviceName": "api",\r\n  "namespace": "default",\r\n  "errorRate": 0.03,\r\n  "p95Latency": 250.5,\r\n  "isHealthy": true,\r\n  "status": "healthy",\r\n  "timestamp": "2024-01-15T10:30:00Z"\r\n}\n'})}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"get-metricscapacity",children:"GET /metrics/capacity"}),"\n",(0,t.jsx)(r.p,{children:"Get cluster capacity report with hot nodes."}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Response"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "timestamp": "2024-01-15T10:30:00Z",\r\n  "totalCpu": 8.0,\r\n  "freeCpu": 3.5,\r\n  "totalMemory": 16.0,\r\n  "freeMemory": 8.2,\r\n  "cpuUtilization": 0.56,\r\n  "memoryUtilization": 0.49,\r\n  "canScale": true,\r\n  "hotNodes": [\r\n    {\r\n      "nodeName": "node-1",\r\n      "cpuUtilization": 0.85,\r\n      "memoryUtilization": 0.78,\r\n      "podCount": 45\r\n    },\r\n    {\r\n      "nodeName": "node-3",\r\n      "cpuUtilization": 0.82,\r\n      "memoryUtilization": 0.81,\r\n      "podCount": 38\r\n    }\r\n  ]\r\n}\n'})}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"get-metricssnapshot",children:"GET /metrics/snapshot"}),"\n",(0,t.jsx)(r.p,{children:"Get all metrics in one call."}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Response"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "timestamp": "2024-01-15T10:30:00Z",\r\n  "services": {\r\n    "api": {\r\n      "serviceName": "api",\r\n      "isLocal": true,\r\n      "stress": {\r\n        "compositeScore": 0.75,\r\n        "stressLevel": "high"\r\n      },\r\n      "rps": {\r\n        "currentRps": 450.0,\r\n        "p50": 400.0,\r\n        "p95": 500.0\r\n      },\r\n      "health": {\r\n        "errorRate": 0.03,\r\n        "p95Latency": 250.5,\r\n        "status": "healthy"\r\n      },\r\n      "podCount": 3\r\n    },\r\n    "payments": {\r\n      "serviceName": "payments",\r\n      "isLocal": false,\r\n      "stress": {\r\n        "compositeScore": 0.0,\r\n        "stressLevel": "none"\r\n      },\r\n      "rps": {\r\n        "currentRps": 320.0,\r\n        "p95": 350.0\r\n      },\r\n      "podCount": 0\r\n    }\r\n  }\r\n}\n'})}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"get-metricsstats",children:"GET /metrics/stats"}),"\n",(0,t.jsx)(r.p,{children:"Get collection performance statistics."}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Response"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "stress": {\r\n    "source": "kubernetes",\r\n    "totalCollections": 1250,\r\n    "successCount": 1248,\r\n    "errorCount": 2,\r\n    "successRate": 0.998,\r\n    "p50Latency": "250ms",\r\n    "p95Latency": "450ms",\r\n    "lastCollectionTime": "2024-01-15T10:30:00Z"\r\n  },\r\n  "rps": {\r\n    "source": "prometheus",\r\n    "totalCollections": 1250,\r\n    "successCount": 1245,\r\n    "successRate": 0.996,\r\n    "p95Latency": "180ms"\r\n  },\r\n  "health": {\r\n    "source": "prometheus",\r\n    "totalCollections": 1250,\r\n    "successCount": 1240,\r\n    "successRate": 0.992,\r\n    "p95Latency": "200ms"\r\n  }\r\n}\n'})}),"\n",(0,t.jsx)(r.h2,{id:"events-emitted",children:"Events Emitted"}),"\n",(0,t.jsx)(r.h3,{id:"stressevent",children:"StressEvent"}),"\n",(0,t.jsx)(r.p,{children:"Emitted when pod composite stress exceeds threshold:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "type": "stress_high",\r\n  "serviceName": "api",\r\n  "namespace": "default",\r\n  "isLocal": true,\r\n  "stressScore": 0.85,\r\n  "currentRps": 450.0,\r\n  "reason": "High stress: 85.0%",\r\n  "timestamp": "2024-01-15T10:30:00Z"\r\n}\n'})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Triggers when:"})}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.code,{children:"stress_high"}),": Composite score \u2265 0.80 (+ hysteresis)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.code,{children:"stress_critical"}),": Composite score \u2265 0.90"]}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Orchestrator action"}),": Traffic shifting strategy"]}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"highrpsevent",children:"HighRPSEvent"}),"\n",(0,t.jsx)(r.p,{children:"Emitted when mirrored service RPS exceeds adaptive P95 threshold:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "type": "rps_high",\r\n  "serviceName": "payments-mirrored",\r\n  "namespace": "default",\r\n  "isLocal": false,\r\n  "stressScore": 0.0,\r\n  "currentRps": 550.0,\r\n  "reason": "High RPS: 550 req/s (P95: 500)",\r\n  "timestamp": "2024-01-15T10:30:00Z"\r\n}\n'})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Triggers when:"})}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Service is mirrored (not local)"}),"\n",(0,t.jsx)(r.li,{children:"Current RPS > P95 + (P95 \xd7 hysteresis)"}),"\n",(0,t.jsx)(r.li,{children:"Example: 550 > 500 + (500 \xd7 0.10) = 550 > 550 \u2713"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Orchestrator action"}),": Deploy service locally"]}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"lowrpsevent",children:"LowRPSEvent"}),"\n",(0,t.jsx)(r.p,{children:"Emitted when local service RPS drops below adaptive P50 threshold:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-json",children:'{\r\n  "type": "rps_low",\r\n  "serviceName": "old-api",\r\n  "namespace": "default",\r\n  "isLocal": true,\r\n  "stressScore": 0.15,\r\n  "currentRps": 8.0,\r\n  "reason": "Low RPS: 8 req/s (P50: 10)",\r\n  "timestamp": "2024-01-15T10:30:00Z"\r\n}\n'})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Triggers when:"})}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Service is local (not mirrored)"}),"\n",(0,t.jsx)(r.li,{children:"Current RPS < P50 - (P50 \xd7 hysteresis)"}),"\n",(0,t.jsx)(r.li,{children:"Example: 8 < 10 - (10 \xd7 0.10) = 8 < 9 \u2713"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Orchestrator action"}),": Mark for deletion"]}),"\n",(0,t.jsx)(r.h2,{id:"deployment",children:"Deployment"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:"apiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: metrics-monitor\r\n  namespace: defog-system\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n      app: metrics-monitor\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: metrics-monitor\r\n    spec:\r\n      serviceAccountName: defog-controller\r\n      containers:\r\n      - name: metrics-monitor\r\n        image: registry.io/defog-metrics-monitor:v2.0.0\r\n        ports:\r\n        - containerPort: 8080\r\n          name: http\r\n        - containerPort: 9090\r\n          name: metrics\r\n        envFrom:\r\n        - configMapRef:\r\n            name: defog-config\r\n        resources:\r\n          requests:\r\n            cpu: 100m\r\n            memory: 200Mi\r\n          limits:\r\n            cpu: 200m\r\n            memory: 400Mi\r\n        livenessProbe:\r\n          httpGet:\r\n            path: /health\r\n            port: 8080\r\n          initialDelaySeconds: 10\r\n          periodSeconds: 30\r\n        readinessProbe:\r\n          httpGet:\r\n            path: /ready\r\n            port: 8080\r\n          initialDelaySeconds: 5\r\n          periodSeconds: 10\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: metrics-monitor\r\n  namespace: defog-system\r\nspec:\r\n  selector:\r\n    app: metrics-monitor\r\n  ports:\r\n  - port: 8080\r\n    targetPort: 8080\r\n    name: http\r\n  - port: 9090\r\n    targetPort: 9090\r\n    name: metrics\n"})}),"\n",(0,t.jsx)(r.h2,{id:"operational-characteristics",children:"Operational Characteristics"}),"\n",(0,t.jsx)(r.h3,{id:"resource-usage-edge-optimized",children:"Resource Usage (Edge-Optimized)"}),"\n",(0,t.jsxs)(r.table,{children:[(0,t.jsx)(r.thead,{children:(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.th,{children:"Metric"}),(0,t.jsx)(r.th,{children:"Small Cluster"}),(0,t.jsx)(r.th,{children:"Medium Cluster"}),(0,t.jsx)(r.th,{children:"Large Cluster"})]})}),(0,t.jsxs)(r.tbody,{children:[(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"CPU"}),(0,t.jsx)(r.td,{children:"80-100m"}),(0,t.jsx)(r.td,{children:"100-150m"}),(0,t.jsx)(r.td,{children:"150-200m"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"Memory"}),(0,t.jsx)(r.td,{children:"150-200Mi"}),(0,t.jsx)(r.td,{children:"200-300Mi"}),(0,t.jsx)(r.td,{children:"300-400Mi"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"API Calls/min"}),(0,t.jsx)(r.td,{children:"6-8"}),(0,t.jsx)(r.td,{children:"10-15"}),(0,t.jsx)(r.td,{children:"15-25"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"Prometheus Queries/min"}),(0,t.jsx)(r.td,{children:"8"}),(0,t.jsx)(r.td,{children:"12"}),(0,t.jsx)(r.td,{children:"16"})]}),(0,t.jsxs)(r.tr,{children:[(0,t.jsx)(r.td,{children:"Network"}),(0,t.jsx)(r.td,{children:"< 50 Kbps"}),(0,t.jsx)(r.td,{children:"< 200 Kbps"}),(0,t.jsx)(r.td,{children:"< 500 Kbps"})]})]})]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Cluster Definitions"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Small: < 50 services, < 10 nodes"}),"\n",(0,t.jsx)(r.li,{children:"Medium: 50-200 services, 10-30 nodes"}),"\n",(0,t.jsx)(r.li,{children:"Large: 200-500 services, 30+ nodes"}),"\n"]}),"\n",(0,t.jsx)(r.h3,{id:"performance",children:"Performance"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Collection cycle"}),": 30 seconds (configurable)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Processing time"}),": < 1 second per cycle (concurrent)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Event latency"}),": < 500ms"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"History retention"}),": Last 5 minutes (50 samples)"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Max services tracked"}),": 500+"]}),"\n"]}),"\n",(0,t.jsx)(r.h3,{id:"key-optimizations",children:"Key Optimizations"}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Concurrent collection"})," - 3 goroutines in parallel"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Batch K8s calls"})," - All pods in one request"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Informers"})," - No polling, instant updates"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Simple percentiles"})," - No interpolation"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Bounded history"})," - 50 samples max (was 100)"]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"monitoring",children:"Monitoring"}),"\n",(0,t.jsx)(r.h3,{id:"prometheus-metrics",children:"Prometheus Metrics"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-promql",children:'# Collection cycle duration\r\nhistogram_quantile(0.95, defog_collection_duration_seconds)\r\n\r\n# Events emitted per type\r\nrate(defog_events_emitted_total{type="stress_high"}[5m])\r\nrate(defog_events_emitted_total{type="rps_high"}[5m])\r\n\r\n# Services monitored\r\ndefog_services_monitored_total\r\n\r\n# Per-service metrics\r\ndefog_service_stress{service="api"} 0.75\r\ndefog_service_rps{service="api"} 450\r\ndefog_service_error_rate{service="api"} 0.03\r\ndefog_service_latency_p95{service="api"} 250\r\n\r\n# Cluster metrics\r\ndefog_cluster_cpu_utilization 0.65\r\ndefog_cluster_memory_utilization 0.58\r\ndefog_cluster_can_scale 1\r\ndefog_cluster_hot_nodes 2\r\n\r\n# Collection stats\r\ndefog_collection_errors_total{source="kubernetes"}\r\ndefog_collection_errors_total{source="prometheus"}\n'})}),"\n",(0,t.jsx)(r.h3,{id:"alerts",children:"Alerts"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'groups:\r\n- name: defog-monitor\r\n  rules:\r\n  # Monitor is not collecting\r\n  - alert: MonitorNotCollecting\r\n    expr: rate(defog_collection_duration_seconds_count[5m]) == 0\r\n    for: 5m\r\n    annotations:\r\n      summary: "Metrics Monitor stopped collecting"\r\n  \r\n  # High collection errors\r\n  - alert: HighCollectionErrors\r\n    expr: rate(defog_collection_errors_total[5m]) > 0.1\r\n    for: 10m\r\n    annotations:\r\n      summary: "High metrics collection error rate"\r\n  \r\n  # Event queue backing up\r\n  - alert: EventQueueBackup\r\n    expr: defog_event_queue_size > 50\r\n    for: 5m\r\n    annotations:\r\n      summary: "Event queue is backing up"\r\n  \r\n  # Cluster capacity low\r\n  - alert: ClusterCapacityLow\r\n    expr: defog_cluster_can_scale == 0\r\n    for: 10m\r\n    annotations:\r\n      summary: "Cluster cannot scale (< 20% free resources)"\r\n  \r\n  # Many hot nodes\r\n  - alert: ManyHotNodes\r\n    expr: defog_cluster_hot_nodes > 3\r\n    for: 15m\r\n    annotations:\r\n      summary: "Multiple nodes running >80% utilization"\n'})}),"\n",(0,t.jsx)(r.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,t.jsx)(r.h3,{id:"issue-no-stress-events-detected",children:"Issue: No Stress Events Detected"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Diagnosis"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:'# Check if metrics-server is installed\r\nkubectl top pods -n default\r\n\r\n# Check monitor logs\r\nkubectl logs -n defog-system deployment/metrics-monitor | grep "stress"\r\n\r\n# Test metrics collection\r\nkubectl exec -n defog-system deployment/metrics-monitor -- \\\r\n  curl localhost:8080/metrics/stress?service=api\n'})}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Common Causes"}),":"]}),"\n",(0,t.jsx)(r.mermaid,{value:"graph TD\r\n    A[No Stress Events] --\x3e B{Metrics Server?}\r\n    B --\x3e|Not Installed| C[Install metrics-server]\r\n    B --\x3e|Installed| D{Pods Have Limits?}\r\n    D --\x3e|No| E[Add resource limits]\r\n    D --\x3e|Yes| F{Threshold Too High?}\r\n    F --\x3e|Yes| G[Lower STRESS_HIGH]\r\n    F --\x3e|No| H{Monitor Running?}\r\n    H --\x3e|No| I[Restart monitor]\r\n    \r\n    style C fill:#ffe1e1\r\n    style E fill:#ffe1e1\r\n    style G fill:#ffffe1\r\n    style I fill:#ffe1e1"}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"issue-no-rps-events-detected",children:"Issue: No RPS Events Detected"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Diagnosis"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:'# Check Prometheus connectivity\r\nkubectl exec -n defog-system deployment/metrics-monitor -- \\\r\n  curl http://prometheus.monitoring.svc:9090/api/v1/query?query=up\r\n\r\n# Check RPS query\r\nkubectl logs -n defog-system deployment/metrics-monitor | grep "RPS"\r\n\r\n# Test RPS collection\r\nkubectl exec -n defog-system deployment/metrics-monitor -- \\\r\n  curl localhost:8080/metrics/rps?service=api\r\n\r\n# Check adaptive thresholds\r\nkubectl logs -n defog-system deployment/metrics-monitor | grep "percentile"\n'})}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Prometheus not configured"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'# Update ConfigMap\r\nPROMETHEUS_URL: "http://prometheus.monitoring.svc:9090"\n'})}),"\n",(0,t.jsxs)(r.ol,{start:"2",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Wrong RPS query"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:"# Update query for your metrics\r\nRPS_QUERY: 'rate(nginx_requests_total{service=\"{{ .ServiceName }}\"}[1m])'\n"})}),"\n",(0,t.jsxs)(r.ol,{start:"3",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Not enough history for adaptive thresholds"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"# Wait 5 minutes for history to build up\r\n# Check history size in logs\n"})}),"\n",(0,t.jsxs)(r.ol,{start:"4",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Services not instrumented"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"# Ensure services expose metrics\r\n# Add Prometheus annotations to pods\n"})}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"issue-high-memory-usage",children:"Issue: High Memory Usage"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Symptoms"}),":"]}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Monitor OOMKilled"}),"\n",(0,t.jsx)(r.li,{children:"Memory usage growing over time"}),"\n"]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Reduce history samples"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'MAX_HISTORY_SAMPLES: "30"  # Was 50\n'})}),"\n",(0,t.jsxs)(r.ol,{start:"2",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Reduce history window"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'RPS_HISTORY_WINDOW: "3m"  # Was 5m\r\nSTRESS_HISTORY_WINDOW: "3m"\n'})}),"\n",(0,t.jsxs)(r.ol,{start:"3",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Limit tracked services"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'MAX_TRACKED_SERVICES: "200"  # Was 500\n'})}),"\n",(0,t.jsxs)(r.ol,{start:"4",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Increase memory limit"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:"resources:\r\n  limits:\r\n    memory: 600Mi  # Was 400Mi\n"})}),"\n",(0,t.jsx)(r.hr,{}),"\n",(0,t.jsx)(r.h3,{id:"issue-adaptive-thresholds-not-working",children:"Issue: Adaptive Thresholds Not Working"}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Diagnosis"}),":"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:'# Check if adaptive mode enabled\r\nkubectl get configmap defog-config -n defog-system -o yaml | grep ADAPTIVE\r\n\r\n# Check history size\r\nkubectl logs -n defog-system deployment/metrics-monitor | grep "history size"\r\n\r\n# Check calculated percentiles\r\ncurl http://metrics-monitor:8080/metrics/rps?service=api\n'})}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Adaptive mode disabled"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'USE_ADAPTIVE_THRESHOLDS: "true"\n'})}),"\n",(0,t.jsxs)(r.ol,{start:"2",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Not enough samples yet"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"# Need at least 20 samples (10 minutes with 30s interval)\r\n# Check current sample count in /metrics/stats\n"})}),"\n",(0,t.jsxs)(r.ol,{start:"3",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Fallback to static thresholds"}),":"]}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'# Ensure static thresholds are reasonable\r\nHIGH_RPS_THRESHOLD: "300.0"\r\nLOW_RPS_THRESHOLD: "10.0"\n'})}),"\n",(0,t.jsx)(r.h2,{id:"advanced-features",children:"Advanced Features"}),"\n",(0,t.jsx)(r.h3,{id:"adaptive-threshold-calculation",children:"Adaptive Threshold Calculation"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:"// Collect RPS history\r\nhistory := []float64{100, 110, 95, 105, 98, 102, 110}\r\n\r\n// Calculate percentiles\r\nP50 := percentile(history, 50)  // 102 RPS\r\nP95 := percentile(history, 95)  // 110 RPS\r\n\r\n// Dynamic thresholds adapt to each service\r\nif currentRPS > P95 + (P95 * hysteresis) {\r\n    emitHighRPSEvent()\r\n}\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Benefits:"})}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"No manual tuning per service"}),"\n",(0,t.jsx)(r.li,{children:"Handles different scales (5 RPS vs 5000 RPS)"}),"\n",(0,t.jsx)(r.li,{children:"Adapts to traffic pattern changes"}),"\n",(0,t.jsx)(r.li,{children:"Prevents false positives during spikes"}),"\n"]}),"\n",(0,t.jsx)(r.h3,{id:"hot-node-detection",children:"Hot Node Detection"}),"\n",(0,t.jsx)(r.p,{children:"Identifies nodes under heavy load for proactive management:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:"// Node with >80% CPU or Memory utilization\r\ntype NodeInfo struct {\r\n    NodeName          string\r\n    CPUUtilization    float64  // 0.85 = 85%\r\n    MemoryUtilization float64  // 0.78 = 78%\r\n    PodCount          int      // Number of pods\r\n}\r\n\r\n// Use cases:\r\n// 1. Avoid scheduling new pods on hot nodes\r\n// 2. Trigger node autoscaling\r\n// 3. Alert operations team\n"})}),"\n",(0,t.jsx)(r.h3,{id:"event-throttling",children:"Event Throttling"}),"\n",(0,t.jsx)(r.p,{children:"Prevents event storms using cooldown periods:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:'// Don\'t emit same event type for same service within 2 minutes\r\nlastEventTime := eventCache["stress_high-api"]\r\nif time.Since(lastEventTime) < 2*time.Minute {\r\n    // Skip event (throttled)\r\n    return\r\n}\r\n\r\n// Also applies hysteresis to thresholds\r\n// StressHigh triggers at 0.80\r\n// But only releases at 0.75 (0.80 - 0.05)\r\n// Prevents oscillation\n'})}),"\n",(0,t.jsx)(r.h3,{id:"composite-stress-scoring",children:"Composite Stress Scoring"}),"\n",(0,t.jsx)(r.p,{children:"Customize CPU/Memory weights per service type:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:"service_overrides:\r\n  # CPU-heavy workload (video encoding, ML)\r\n  video-processor:\r\n    cpu_weight: 0.9\r\n    memory_weight: 0.1\r\n  \r\n  # Memory-heavy workload (cache, database)\r\n  redis-cache:\r\n    cpu_weight: 0.3\r\n    memory_weight: 0.7\r\n  \r\n  # Balanced workload (web services)\r\n  web-api:\r\n    cpu_weight: 0.7\r\n    memory_weight: 0.3\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Score calculation:"})}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:"compositeScore = (cpuUsage * cpuWeight) + (memUsage * memWeight)\r\n\r\n// Example: Video processor at CPU=0.90, Mem=0.50\r\nscore = (0.90 * 0.9) + (0.50 * 0.1) = 0.81 + 0.05 = 0.86 \u2192 High stress!\r\n\r\n// Example: Redis cache at CPU=0.50, Mem=0.90\r\nscore = (0.50 * 0.3) + (0.90 * 0.7) = 0.15 + 0.63 = 0.78 \u2192 Medium stress\n"})}),"\n",(0,t.jsx)(r.h3,{id:"health-based-alerting",children:"Health-Based Alerting"}),"\n",(0,t.jsx)(r.p,{children:"Correlate stress with actual service health:"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-go",children:"// Only alert if service is both stressed AND unhealthy\r\nif stress > 0.80 && (errorRate > 0.05 || latency > 1000) {\r\n    // Service is legitimately struggling\r\n    emitCriticalAlert()\r\n} else if stress > 0.80 && errorRate < 0.01 {\r\n    // High stress but service is fine\r\n    // Maybe just handling load well\r\n    emitInfoAlert()\r\n}\n"})}),"\n",(0,t.jsx)(r.h2,{id:"integration-points",children:"Integration Points"}),"\n",(0,t.jsx)(r.h3,{id:"with-orchestrator",children:"With Orchestrator"}),"\n",(0,t.jsx)(r.mermaid,{value:'sequenceDiagram\r\n    participant M as Metrics Monitor\r\n    participant O as Orchestrator\r\n    \r\n    Note over M: Detect high stress (0.85)\r\n    M->>O: POST /events<br/>{type: "stress_high", score: 0.85}\r\n    O--\x3e>M: 202 Accepted\r\n    Note over O: Trigger stress strategy<br/>(shift traffic)\r\n    \r\n    Note over M: Detect high RPS (550 > P95)\r\n    M->>O: POST /events<br/>{type: "rps_high", rps: 550}\r\n    O--\x3e>M: 202 Accepted\r\n    Note over O: Trigger RPS strategy<br/>(deploy locally)\r\n    \r\n    Note over M: Detect health degradation\r\n    M->>O: POST /events<br/>{type: "health_degraded"}\r\n    O--\x3e>M: 202 Accepted\r\n    Note over O: Investigate and scale'}),"\n",(0,t.jsx)(r.h3,{id:"with-prometheus",children:"With Prometheus"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:'# Queries used by Metrics Monitor\r\n\r\n# RPS tracking\r\nrate(http_requests_total{service="api"}[1m])\r\n\r\n# Error rate\r\nrate(http_requests_total{service="api", status=~"5.."}[1m]) / \r\nrate(http_requests_total{service="api"}[1m])\r\n\r\n# P95 Latency\r\nhistogram_quantile(0.95, \r\n  rate(http_request_duration_seconds_bucket{service="api"}[1m])\r\n)\n'})}),"\n",(0,t.jsx)(r.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Start with adaptive thresholds"})," - Let the system learn traffic patterns"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Monitor collection stats"})," - Check ",(0,t.jsx)(r.code,{children:"/metrics/stats"})," for success rates"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Tune per-service weights"})," - CPU-heavy vs memory-heavy workloads"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Set appropriate cooldowns"})," - Balance responsiveness vs event storms"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Watch hot nodes"})," - Proactively manage cluster capacity"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Use health metrics"})," - Correlate stress with actual service issues"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Keep history small"})," - 50 samples sufficient for edge clusters"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Alert on collection failures"})," - Critical for system reliability"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Test adaptive thresholds"})," - Verify with load testing"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Monitor percentile drift"})," - Track how P50/P95 change over time"]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"source-code-reference",children:"Source Code Reference"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{children:"internal/monitor/\r\n\u251c\u2500\u2500 monitor.go         # Main orchestration with concurrent collection\r\n\u251c\u2500\u2500 server.go          # HTTP API handlers\r\n\r\ninternal/collector/\r\n\u251c\u2500\u2500 unified.go         # Concurrent collector (3 goroutines)\r\n\u251c\u2500\u2500 stress.go          # Stress collection (batch K8s + informers)\r\n\u251c\u2500\u2500 rps.go            # RPS collection (adaptive percentiles)\r\n\u251c\u2500\u2500 health.go         # Health collection (error rate + latency)\r\n\u2514\u2500\u2500 capacity.go       # Cluster capacity & hot nodes\r\n\r\ninternal/analyzer/\r\n\u2514\u2500\u2500 event.go          # Event analysis & generation\r\n\r\npkg/types/\r\n\u2514\u2500\u2500 types.go          # Core data structures\r\n\r\npkg/config/\r\n\u251c\u2500\u2500 config.go         # Configuration types\r\n\u2514\u2500\u2500 watcher.go        # Hot reload watcher\r\n\r\npkg/metrics/\r\n\u2514\u2500\u2500 registry.go       # Prometheus metrics\n"})}),"\n",(0,t.jsx)(r.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,t.jsx)(r.h3,{id:"for-small-edge-clusters--50-services",children:"For Small Edge Clusters (< 50 services)"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'# Optimize for minimal resource usage\r\nCHECK_INTERVAL: "45s"           # Less frequent\r\nMAX_HISTORY_SAMPLES: "30"       # Smaller buffer\r\nMAX_TRACKED_SERVICES: "100"\r\nresources:\r\n  limits:\r\n    cpu: 100m\r\n    memory: 200Mi\n'})}),"\n",(0,t.jsx)(r.h3,{id:"for-medium-clusters-50-200-services",children:"For Medium Clusters (50-200 services)"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'# Balanced configuration\r\nCHECK_INTERVAL: "30s"           # Standard\r\nMAX_HISTORY_SAMPLES: "50"       # Default\r\nMAX_TRACKED_SERVICES: "500"\r\nresources:\r\n  limits:\r\n    cpu: 200m\r\n    memory: 400Mi\n'})}),"\n",(0,t.jsx)(r.h3,{id:"for-large-clusters-200-services",children:"For Large Clusters (200+ services)"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'# Optimize for throughput\r\nCHECK_INTERVAL: "20s"           # More frequent\r\nMAX_HISTORY_SAMPLES: "50"       # Keep bounded\r\nMAX_TRACKED_SERVICES: "1000"\r\nresources:\r\n  limits:\r\n    cpu: 300m\r\n    memory: 600Mi\n'})}),"\n",(0,t.jsx)(r.h2,{id:"migration-guide",children:"Migration Guide"}),"\n",(0,t.jsx)(r.h3,{id:"from-static-to-adaptive-thresholds",children:"From Static to Adaptive Thresholds"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-yaml",children:'# Step 1: Enable adaptive mode\r\nUSE_ADAPTIVE_THRESHOLDS: "true"\r\n\r\n# Step 2: Keep static as fallback\r\nHIGH_RPS_THRESHOLD: "300.0"  # Used until history builds\r\nLOW_RPS_THRESHOLD: "10.0"\r\n\r\n# Step 3: Wait for history (10-15 minutes)\r\n# Monitor logs for "calculated percentiles"\r\n\r\n# Step 4: Verify adaptive thresholds\r\ncurl http://metrics-monitor:8080/metrics/rps?service=your-service\r\n# Check P50 and P95 values are reasonable\r\n\r\n# Step 5: (Optional) Remove static thresholds once stable\n'})}),"\n",(0,t.jsx)(r.h3,{id:"from-v1-to-v2-monitor",children:"From v1 to v2 Monitor"}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Breaking changes:"})}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsx)(r.li,{children:"Event structure simplified (no priority field)"}),"\n",(0,t.jsxs)(r.li,{children:["New endpoints: ",(0,t.jsx)(r.code,{children:"/metrics/health"}),", ",(0,t.jsx)(r.code,{children:"/metrics/stats"})]}),"\n",(0,t.jsx)(r.li,{children:"ConfigMap keys changed (see Configuration section)"}),"\n"]}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.strong,{children:"Migration steps:"})}),"\n",(0,t.jsxs)(r.ol,{children:["\n",(0,t.jsx)(r.li,{children:(0,t.jsx)(r.strong,{children:"Update ConfigMap:"})}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"kubectl apply -f new-defog-config.yaml\n"})}),"\n",(0,t.jsxs)(r.ol,{start:"2",children:["\n",(0,t.jsx)(r.li,{children:(0,t.jsx)(r.strong,{children:"Deploy v2 monitor:"})}),"\n"]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-bash",children:"kubectl set image deployment/metrics-monitor \\\r\n  metrics-monitor=registry.io/defog-metrics-monitor:v2.0.0\n"})}),"\n",(0,t.jsxs)(r.ol,{start:"3",children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Update Orchestrator"})," to handle new event structure"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Monitor for errors"})," in first 15 minutes"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.strong,{children:"Verify adaptive thresholds"})," are calculating"]}),"\n"]}),"\n",(0,t.jsx)(r.h2,{id:"related-documentation",children:"Related Documentation"}),"\n",(0,t.jsxs)(r.ul,{children:["\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"../dual-strategy",children:"Dual-Strategy Design"})," - How metrics drive decisions"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"./orchestrator",children:"Orchestrator"})," - Event consumer"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"../configuration",children:"Configuration Guide"})," - Threshold tuning"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"../troubleshooting",children:"Troubleshooting"})," - Common issues"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"../architecture",children:"Architecture Overview"})," - System design"]}),"\n",(0,t.jsxs)(r.li,{children:[(0,t.jsx)(r.a,{href:"../deployment",children:"Deployment Guide"})," - Production setup"]}),"\n"]})]})}function h(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>l,x:()=>o});var s=n(6540);const t={},i=s.createContext(t);function l(e){const r=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function o(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),s.createElement(i.Provider,{value:r},e.children)}}}]);